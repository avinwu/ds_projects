{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loan Default Prediction - Imperial College London\n",
    "\n",
    "- Refer to 2nd Place Solution for Complete steps. Steps are documented below\n",
    "- Here training data was split and we did default prediction using GradientBoostingClassifier\n",
    "\n",
    "https://www.kaggle.com/c/loan-default-prediction/data\n",
    "\n",
    "## Overview\n",
    "This competition asks you to determine whether a loan will default, as well as the loss incurred if it does default. Unlike traditional finance-based approaches to this problem, where one distinguishes between good or bad counterparties in a binary way, we seek to anticipate and incorporate both the default and the severity of the losses that result. In doing so, we are building a bridge between traditional banking, where we are looking at reducing the consumption of economic capital, to an asset-management perspective, where we optimize on the risk to the financial investor.\n",
    "\n",
    "This competition is sponsored by researchers at Imperial College London.\n",
    "\n",
    "## Data Description\n",
    "\n",
    "This data corresponds to a set of financial transactions associated with individuals. The data has been standardized, de-trended, and anonymized. You are provided with over two hundred thousand observations and nearly 800 features.  Each observation is independent from the previous. \n",
    "\n",
    "For each observation, it was recorded whether a default was triggered. In case of a default, the loss was measured. This quantity lies between 0 and 100. It has been normalised, considering that the notional of each transaction at inception is 100. For example, a loss of 60 means that only 40 is reimbursed. If the loan did not default, the loss was 0. You are asked to predict the losses for each observation in the test set.\n",
    "\n",
    "Missing feature values have been kept as is, so that the competing teams can really use the maximum data available, implementing a strategy to fill the gaps if desired. Note that some variables may be categorical (e.g. f776 and f777).\n",
    "\n",
    "The competition sponsor has worked to remove time-dimensionality from the data. However, the observations are still listed in order from old to new in the training set. In the test set they are in random order.\n",
    "\n",
    "\n",
    "\n",
    "## Functional Terms and Definitions\n",
    "- **Probability of Default (PD)**\n",
    "- **Loss Given Default (LGD)**\n",
    "- **Expected Loss (EL)**\n",
    "- **Economic Capital (EC)**\n",
    "\n",
    "## Steps\n",
    "\n",
    "### Data Load and Preprocess\n",
    "\n",
    "- 1) load_train_fs : Load Training data\n",
    "- 2) load_test_fs  : Load Test Data\n",
    "- 3) train_type    : Process Training data\n",
    "- 4) test_type\t : Process Test data\n",
    "\n",
    "### Feature Selection\n",
    "- 5) getTopFeatures : Get Top 100 Features\n",
    "\n",
    "### Feature Extraction\n",
    "- 6) feature_minus_pair_list\n",
    "- 7) feature_plus_pair_list\n",
    "- 8) feature_mul_pair_list\n",
    "- 9) feature_divide_pair_list\n",
    "- 10) feature_pair_sub_mul_list\n",
    "\n",
    "- 11) Generate Labels: toLabels()\n",
    "\n",
    "### Modeling and Evaluation\n",
    "- 12) Fit Model : GradientBoostingClassifier()\n",
    "- 13) Predict : gbc_svr_predict2()\n",
    "- 14) Evaluate: get_evaluation_matrices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_evaluation_matrices(y_actual, y_predicted, y_pred_prob_pos):\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.metrics import classification_report,confusion_matrix,accuracy_score,roc_curve,roc_auc_score,auc\n",
    "\n",
    "    # Accuracy Score\n",
    "    print('----------------- Accuracy Score ----------------------------------')\n",
    "    print('Accuracy Score : ', accuracy_score(y_actual, y_predicted).round(4))\n",
    "\n",
    "    # Confusion Matrix\n",
    "    print('\\n----------------- Confusion Matrix --------------------------------')\n",
    "    \n",
    "    cmx = confusion_matrix(y_actual, y_predicted)\n",
    "    '''\n",
    "    f, ax = plt.subplots(figsize=(10, 6))\n",
    "    sns.heatmap(cmx, annot=True, fmt=\"d\", linewidths=.5, ax=ax)\n",
    "    plt.title(\"Confusion Matrix\", fontsize=20)\n",
    "    ax.set_yticks(np.arange(cmx.shape[0]) + 0.5, minor=False)\n",
    "    ax.set_xticklabels(\"\")\n",
    "    #ax.set_yticklabels(['Fraudaulant Transaction', 'Valid Transaction'], fontsize=16, rotation=360)\n",
    "    plt.show()\n",
    "    '''\t\n",
    "    print(cmx)\n",
    "\n",
    "\n",
    "    tn = cmx[0,0]  # True  Negative\n",
    "    fp = cmx[0,1]  # False Positive\n",
    "    fn = cmx[1,0]  # False Negative\n",
    "    tp = cmx[1,1]  # True  Positive\n",
    "\n",
    "    print('\\n----------------- TP,FP,TN,FN -------------------------------------')\n",
    "    print('True  Positive : ', tp)\n",
    "    print('False Positive : ', fp)\n",
    "    print('True  Negative : ', tn)\n",
    "    print('False Negative : ', fn)\n",
    "    print('Number of Correct Predictions   (TP + TN) : ', tp + tn)\n",
    "    print('Number of Incorrect Predictions (FP + FN) : ', fp + fn)\n",
    "\n",
    "    print('\\n----------------- Precision/Recall/F1-Score -----------------------')\n",
    "    precision = tp/(tp + fp)\n",
    "    recall = tp/(tp + fn)\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    print('Precision          : ', precision.round(4))\n",
    "    print('Recall/Sensitivity : ', recall.round(4))\n",
    "    print('F1 Score           : ', f1_score.round(4))\n",
    "\n",
    "\n",
    "    # Classification Report\n",
    "    print('\\n----------------- Classification Report ---------------------------')\n",
    "    print(classification_report(y_actual, y_predicted))\n",
    "\n",
    "    \n",
    "    #### ROC Curve\n",
    "    print('\\n----------------- ROC Curve ---------------------------------------')\n",
    "    '''\n",
    "    - Every prediction the classifier makes has an associated probability.\n",
    "    - Default probability threshold in scikit-learn is 50%\n",
    "    - By default if the probability is \n",
    "        - more than 50%, then the will predict the data point belonging to positive class.\n",
    "        - less than 50%, then the will predict the data point belonging to negative class.\n",
    "    '''\n",
    "\n",
    "    # Calculate the roc metrics\n",
    "    fpr, tpr, thresholds = roc_curve(y_actual, y_pred_prob_pos)\n",
    "\n",
    "    # Plot the ROC curve\n",
    "    plt.plot(fpr,tpr);\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.plot([0, 1], [0, 1], \"k--\")\n",
    "    plt.show()\n",
    "\n",
    "    print('\\n----------------- ROC AUC Score -----------------------------------')\n",
    "    # Print the AUC\n",
    "    print('ROC AUC Score : ', roc_auc_score(y_actual,y_pred_prob_pos).round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import collections\n",
    "import numpy as np\n",
    "import time\n",
    "import operator\n",
    "from scipy.io import mmread, mmwrite\n",
    "from random import randint\n",
    "#from sklearn import cross_validation #AP\n",
    "from sklearn import linear_model\n",
    "#from sklearn.grid_search import GridSearchCV #AP\n",
    "from sklearn import preprocessing as pp\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier\n",
    "from sklearn.ensemble import  RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "#from sklearn.decomposition import ProbabilisticPCA, KernelPCA #AP\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression, Ridge, Lasso, ElasticNet\n",
    "import scipy.stats as stats\n",
    "from sklearn import tree\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import auc, f1_score\n",
    "#from sklearn.gaussian_process import GaussianProcess\n",
    "#import features\n",
    "\n",
    "# working directory\n",
    "#dir = '.' # AP\n",
    "#cwd = os.getcwd() #AP\n",
    "#dir = os.path.dirname(cwd) #AP\n",
    "\n",
    "label_index = 770"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train data\n",
    "def load_train_fs():\n",
    "    # In the validation process, the training data was randomly shuffled firstly.\n",
    "    # For the prediction process, there is no need to shuffle the dataset.\n",
    "    # Owing to out of memory problem, Gaussian process only use part of training data, the prediction of gaussian process\n",
    "    # may be a little different from the model,which the training data was shuffled.\n",
    "    train_fs = np.genfromtxt(open(dir + '/train_v2.csv', 'rb'), delimiter=',', skip_header=1)\n",
    "    col_mean = stats.nanmean(train_fs, axis=0)\n",
    "    inds = np.where(np.isnan(train_fs))\n",
    "    train_fs[inds] = np.take(col_mean, inds[1])\n",
    "    train_fs[np.isinf(train_fs)] = 0\n",
    "    return train_fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test data\n",
    "def load_test_fs():\n",
    "    test_fs = np.genfromtxt(open(dir + '/test_v2.csv', 'rb'), delimiter=',', skip_header=1)\n",
    "    col_mean = stats.nanmean(test_fs, axis=0)\n",
    "    inds = np.where(np.isnan(test_fs))\n",
    "    test_fs[inds] = np.take(col_mean, inds[1])\n",
    "    test_fs[np.isinf(test_fs)] = 0\n",
    "    return test_fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract features from test data\n",
    "def test_type(test_fs):\n",
    "    x_Test = test_fs[:, range(1, label_index)]\n",
    "    return x_Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract features from train data\n",
    "def train_type(train_fs):\n",
    "    train_x = train_fs[:, range(1, label_index)]\n",
    "    train_y = train_fs[:, -1]\n",
    "    return train_x, train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the loss to the binary form\n",
    "def toLabels(train_y):\n",
    "    labels = np.zeros(len(train_y))\n",
    "    labels[train_y > 0] = 1\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the output file based to the predictions\n",
    "def output_preds(preds):\n",
    "    out_file = dir + '/output.csv'\n",
    "    fs = open(out_file, 'w')\n",
    "    fs.write('id,loss\\n')\n",
    "    for i in range(len(preds)):\n",
    "        if preds[i] > 100:\n",
    "            preds[i] = 100\n",
    "        elif preds[i] < 0:\n",
    "            preds[i] = 0\n",
    "        strs = str(i + 105472) + ',' + str(np.float(preds[i]))\n",
    "        fs.write(strs + '\\n');\n",
    "    fs.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the top feature indexes by invoking f_regression \n",
    "def getTopFeatures(train_x, train_y, n_features=100):\n",
    "    f_val, p_val = f_regression(train_x,train_y)\n",
    "    f_val_dict = {}\n",
    "    p_val_dict = {}\n",
    "    for i in range(len(f_val)):\n",
    "        if math.isnan(f_val[i]):\n",
    "            f_val[i] = 0.0\n",
    "        f_val_dict[i] = f_val[i]\n",
    "        if math.isnan(p_val[i]):\n",
    "            p_val[i] = 0.0\n",
    "        p_val_dict[i] = p_val[i]\n",
    "    \n",
    "    sorted_f = sorted(f_val_dict.items(), key=operator.itemgetter(1),reverse=True)\n",
    "    sorted_p = sorted(p_val_dict.items(), key=operator.itemgetter(1),reverse=True)\n",
    "    \n",
    "    feature_indexs = []\n",
    "    for i in range(0,n_features):\n",
    "        feature_indexs.append(sorted_f[i][0])\n",
    "    \n",
    "    return feature_indexs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the new data, based on which features are generated, and used\n",
    "def get_data(train_x, feature_indexs, feature_minus_pair_list=[], feature_plus_pair_list=[],\n",
    "            feature_mul_pair_list=[], feature_divide_pair_list = [], feature_pair_sub_mul_list=[],\n",
    "            feature_pair_plus_mul_list = [],feature_pair_sub_divide_list = [], feature_minus2_pair_list = [],feature_mul2_pair_list=[], \n",
    "            feature_sub_square_pair_list=[], feature_square_sub_pair_list=[],feature_square_plus_pair_list=[]):\n",
    "    sub_train_x = train_x[:,feature_indexs]\n",
    "    \n",
    "    print('feature_minus_pair_list')\n",
    "    for i in range(len(feature_minus_pair_list)):\n",
    "        ind_i = feature_minus_pair_list[i][0]\n",
    "        ind_j = feature_minus_pair_list[i][1]\n",
    "        sub_train_x = np.column_stack((sub_train_x, train_x[:,ind_i]-train_x[:,ind_j]))\n",
    "    \n",
    "    print('feature_plus_pair_list')\n",
    "    for i in range(len(feature_plus_pair_list)):\n",
    "        ind_i = feature_plus_pair_list[i][0]\n",
    "        ind_j = feature_plus_pair_list[i][1]\n",
    "        sub_train_x = np.column_stack((sub_train_x, train_x[:,ind_i] + train_x[:,ind_j]))\n",
    "    \n",
    "    print('feature_mul_pair_list')\n",
    "    for i in range(len(feature_mul_pair_list)):\n",
    "        ind_i = feature_mul_pair_list[i][0]\n",
    "        ind_j = feature_mul_pair_list[i][1]\n",
    "        sub_train_x = np.column_stack((sub_train_x, train_x[:,ind_i] * train_x[:,ind_j]))\n",
    "    \n",
    "    print('feature_divide_pair_list')\n",
    "    for i in range(len(feature_divide_pair_list)):\n",
    "        ind_i = feature_divide_pair_list[i][0]\n",
    "        ind_j = feature_divide_pair_list[i][1]\n",
    "        sub_train_x = np.column_stack((sub_train_x, train_x[:,ind_i] / train_x[:,ind_j]))\n",
    "    \n",
    "    print('feature_pair_sub_mul_list')\n",
    "    for i in range(len(feature_pair_sub_mul_list)):\n",
    "        ind_i = feature_pair_sub_mul_list[i][0]\n",
    "        ind_j = feature_pair_sub_mul_list[i][1]\n",
    "        ind_k = feature_pair_sub_mul_list[i][2]\n",
    "        sub_train_x = np.column_stack((sub_train_x, (train_x[:,ind_i]-train_x[:,ind_j]) * train_x[:,ind_k]))\n",
    "        \n",
    "    return sub_train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use gbm classifier to predict whether the loan defaults or not\n",
    "def gbc_classify(train_x, train_y):\n",
    "    feature_indexs = getTopFeatures(train_x, train_y)\n",
    "    sub_x_Train = get_data(train_x, feature_indexs[:16], features.feature_pair_sub_list\n",
    "                ,features.feature_pair_plus_list, features.feature_pair_mul_list, features.feature_pair_divide_list[:20],\n",
    "                features.feature_pair_sub_mul_list[:20])\n",
    "    labels = toLabels(train_y)\n",
    "    gbc = GradientBoostingClassifier(n_estimators=3000, max_depth=8)\n",
    "    gbc.fit(sub_x_Train, labels)\n",
    "    return gbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use svm to predict the loss, based on the result of gbm classifier\n",
    "def gbc_svr_predict_part(gbc, train_x, train_y, test_x, feature_pair_sub_list,\n",
    "                         feature_pair_plus_list, feature_pair_mul_list, feature_pair_divide_list,\n",
    "                         feature_pair_sub_mul_list, feature_pair_sub_list_sf, feature_pair_plus_list2):\n",
    "    feature_indexs = getTopFeatures(train_x, train_y)\n",
    "    sub_x_Train = get_data(train_x, feature_indexs[:16], feature_pair_sub_list\n",
    "                           , feature_pair_plus_list, feature_pair_mul_list, feature_pair_divide_list[:20],\n",
    "                           feature_pair_sub_mul_list[:20])\n",
    "    sub_x_Test = get_data(test_x, feature_indexs[:16], feature_pair_sub_list\n",
    "                          , feature_pair_plus_list, feature_pair_mul_list, feature_pair_divide_list[:20],\n",
    "                          feature_pair_sub_mul_list[:20])\n",
    "    pred_labels = gbc.predict(sub_x_Test)\n",
    "\n",
    "    pred_probs = gbc.predict_proba(sub_x_Test)[:, 1]\n",
    "\n",
    "    ind_test = np.where(pred_probs > 0.55)[0]\n",
    "\n",
    "    ind_train = np.where(train_y > 0)[0]\n",
    "    ind_train0 = np.where(train_y == 0)[0]\n",
    "\n",
    "    preds_all = np.zeros([len(sub_x_Test)])\n",
    "\n",
    "    flag = (sub_x_Test[:, 16] >= 1)\n",
    "    ind_tmp0 = np.where(flag)[0]\n",
    "\n",
    "    ind_tmp = np.where(~flag)[0]\n",
    "\n",
    "    sub_x_Train = get_data(train_x, feature_indexs[:100], feature_pair_sub_list_sf\n",
    "                           , feature_pair_plus_list2[:100], feature_pair_mul_list[:40], feature_pair_divide_list,\n",
    "                           feature_pair_sub_mul_list)\n",
    "    sub_x_Test = get_data(test_x, feature_indexs[:100], feature_pair_sub_list_sf\n",
    "                          , feature_pair_plus_list2[:100], feature_pair_mul_list[:40], feature_pair_divide_list,\n",
    "                          feature_pair_sub_mul_list)\n",
    "    sub_x_Train[:, 101] = np.log(1 - sub_x_Train[:, 101])\n",
    "    sub_x_Test[ind_tmp, 101] = np.log(1 - sub_x_Test[ind_tmp, 101])\n",
    "    scaler = pp.StandardScaler()\n",
    "    scaler.fit(sub_x_Train)\n",
    "    sub_x_Train = scaler.transform(sub_x_Train)\n",
    "    sub_x_Test[ind_tmp] = scaler.transform(sub_x_Test[ind_tmp])\n",
    "\n",
    "    svr = SVR(C=16, kernel='rbf', gamma=0.000122)\n",
    "\n",
    "    svr.fit(sub_x_Train[ind_train], np.log(train_y[ind_train]))\n",
    "    preds = svr.predict(sub_x_Test[ind_test])\n",
    "    preds_all[ind_test] = np.power(np.e, preds)\n",
    "    preds_all[ind_tmp0] = 0\n",
    "    return preds_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gbc_predict_part(gbc, train_x, train_y, test_x, feature_pair_sub_list,\n",
    "                          feature_pair_plus_list, feature_pair_mul_list, feature_pair_divide_list,\n",
    "                          feature_pair_sub_mul_list, feature_pair_sub_list_sf, feature_pair_plus_list2):\n",
    "    feature_indexs = getTopFeatures(train_x, train_y)\n",
    "    sub_x_Train = get_data(train_x, feature_indexs[:16], feature_pair_sub_list\n",
    "                           , feature_pair_plus_list, feature_pair_mul_list, feature_pair_divide_list[:20],\n",
    "                           feature_pair_sub_mul_list[:20])\n",
    "    sub_x_Test = get_data(test_x, feature_indexs[:16], feature_pair_sub_list\n",
    "                          , feature_pair_plus_list, feature_pair_mul_list, feature_pair_divide_list[:20],\n",
    "                          feature_pair_sub_mul_list[:20])\n",
    "    pred_labels = gbc.predict(sub_x_Test)\n",
    "\n",
    "    pred_probs = gbc.predict_proba(sub_x_Test)[:, 1]\n",
    "\n",
    "    return pred_labels, pred_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use gbm regression to predict the loss, based on the result of gbm classifier\n",
    "def gbc_gbr_predict_part(gbc, train_x, train_y, test_x, feature_pair_sub_list,\n",
    "                         feature_pair_plus_list, feature_pair_mul_list, feature_pair_divide_list,\n",
    "                         feature_pair_sub_mul_list, feature_pair_sub_list2):\n",
    "    feature_indexs = getTopFeatures(train_x, train_y)\n",
    "    sub_x_Train = get_data(train_x, feature_indexs[:16], feature_pair_sub_list\n",
    "                           , feature_pair_plus_list, feature_pair_mul_list, feature_pair_divide_list[:20],\n",
    "                           feature_pair_sub_mul_list[:20])\n",
    "    sub_x_Test = get_data(test_x, feature_indexs[:16], feature_pair_sub_list\n",
    "                          , feature_pair_plus_list, feature_pair_mul_list, feature_pair_divide_list[:20],\n",
    "                          feature_pair_sub_mul_list[:20])\n",
    "    pred_labels = gbc.predict(sub_x_Test)\n",
    "\n",
    "    pred_probs = gbc.predict_proba(sub_x_Test)[:, 1]\n",
    "\n",
    "    ind_test = np.where(pred_probs > 0.55)[0]\n",
    "\n",
    "    ind_train = np.where(train_y > 0)[0]\n",
    "    ind_train0 = np.where(train_y == 0)[0]\n",
    "\n",
    "    preds_all = np.zeros([len(sub_x_Test)])\n",
    "\n",
    "    flag = (sub_x_Test[:, 16] >= 1)\n",
    "    ind_tmp0 = np.where(flag)[0]\n",
    "\n",
    "    ind_tmp = np.where(~flag)[0]\n",
    "\n",
    "    sub_x_Train = get_data(train_x, feature_indexs[:16], feature_pair_sub_list2[:70]\n",
    "                           , feature_pair_plus_list, feature_pair_mul_list, feature_pair_divide_list,\n",
    "                           feature_pair_sub_mul_list)\n",
    "    sub_x_Test = get_data(test_x, feature_indexs[:16], feature_pair_sub_list2[:70]\n",
    "                          , feature_pair_plus_list, feature_pair_mul_list, feature_pair_divide_list,\n",
    "                          feature_pair_sub_mul_list)\n",
    "\n",
    "    scaler = pp.StandardScaler()\n",
    "    scaler.fit(sub_x_Train)\n",
    "    sub_x_Train = scaler.transform(sub_x_Train)\n",
    "    sub_x_Test[ind_tmp] = scaler.transform(sub_x_Test[ind_tmp])\n",
    "\n",
    "    gbr1000 = GradientBoostingRegressor(n_estimators=1300, max_depth=4, subsample=0.5, learning_rate=0.05)\n",
    "\n",
    "    gbr1000.fit(sub_x_Train[ind_train], np.log(train_y[ind_train]))\n",
    "    preds = gbr1000.predict(sub_x_Test[ind_test])\n",
    "    preds_all[ind_test] = np.power(np.e, preds)\n",
    "    preds_all[ind_tmp0] = 0\n",
    "    return preds_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the loss based on the Gaussian process regressor, which has been trained\n",
    "def gp_predict(clf, x_Test):\n",
    "    size = len(x_Test)\n",
    "    part_size = 3000\n",
    "    cnt = (size - 1) / part_size + 1\n",
    "    preds = []\n",
    "    for i in range(cnt):\n",
    "        if i < cnt - 1:\n",
    "            pred_part = clf.predict(x_Test[i * part_size: (i + 1) * part_size])\n",
    "        else:\n",
    "            pred_part = clf.predict(x_Test[i * part_size: size])\n",
    "        preds.extend(pred_part)\n",
    "    return np.power(np.e, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the gaussian process regressor\n",
    "def gbc_gp_predict_part(sub_x_Train, train_y, sub_x_Test_part):\n",
    "    # Owing to out of memory, the model was trained by part of training data\n",
    "    # Attention, this part was trained on the ram of more than 96G\n",
    "    sub_x_Train[:, 16] = np.log(1 - sub_x_Train[:, 16])\n",
    "    scaler = pp.StandardScaler()\n",
    "    scaler.fit(sub_x_Train)\n",
    "    sub_x_Train = scaler.transform(sub_x_Train)\n",
    "    ind_train = np.where(train_y > 0)[0]\n",
    "    part_size = int(0.7 * len(ind_train))\n",
    "    gp = GaussianProcess(theta0=1e-3, thetaL=1e-5, thetaU=10, corr='absolute_exponential')\n",
    "    gp.fit(sub_x_Train[ind_train[:part_size]], np.log(train_y[ind_train[:part_size]]))\n",
    "    flag = (sub_x_Test_part[:, 16] >= 1)\n",
    "    ind_tmp0 = np.where(flag)[0]\n",
    "    ind_tmp = np.where(~flag)[0]\n",
    "    sub_x_Test_part[ind_tmp, 16] = np.log(1 - sub_x_Test_part[ind_tmp, 16])\n",
    "    sub_x_Test_part[ind_tmp] = scaler.transform(sub_x_Test_part[ind_tmp])\n",
    "    gp_preds_tmp = gp_predict(gp, sub_x_Test_part[ind_tmp])\n",
    "    gp_preds = np.zeros(len(sub_x_Test_part))\n",
    "    gp_preds[ind_tmp] = gp_preds_tmp\n",
    "    return gp_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use gbm classifier to predict whether the loan defaults or not, then invoke the function gbc_gp_predict_part\n",
    "def gbc_gp_predict(train_x, train_y, test_x):\n",
    "    feature_indexs = getTopFeatures(train_x, train_y)\n",
    "    sub_x_Train = get_data(train_x, feature_indexs[:16], features.feature_pair_sub_list\n",
    "                           , features.feature_pair_plus_list, features.feature_pair_mul_list,\n",
    "                           features.feature_pair_divide_list[:20])\n",
    "    sub_x_Test = get_data(test_x, feature_indexs[:16], features.feature_pair_sub_list\n",
    "                          , features.feature_pair_plus_list, features.feature_pair_mul_list,\n",
    "                          features.feature_pair_divide_list[:20])\n",
    "    labels = toLabels(train_y)\n",
    "    gbc = GradientBoostingClassifier(n_estimators=3000, max_depth=9)\n",
    "    gbc.fit(sub_x_Train, labels)\n",
    "    pred_probs = gbc.predict_proba(sub_x_Test)[:, 1]\n",
    "    ind_test = np.where(pred_probs > 0.55)[0]\n",
    "    gp_preds_part = gbc_gp_predict_part(sub_x_Train, train_y, sub_x_Test[ind_test])\n",
    "    gp_preds = np.zeros(len(test_x))\n",
    "    gp_preds[ind_test] = gp_preds_part\n",
    "    return gp_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invoke the function gbc_svr_predict_part\n",
    "def gbc_svr_predict(gbc, train_x, train_y, test_x):\n",
    "    svr_preds = gbc_svr_predict_part(gbc, train_x, train_y, test_x, features.feature_pair_sub_list,\n",
    "                                     features.feature_pair_plus_list,\n",
    "                                     features.feature_pair_mul_list, features.feature_pair_divide_list,\n",
    "                                     features.feature_pair_sub_mul_list, features.feature_pair_sub_list_sf,\n",
    "                                     features.feature_pair_plus_list2)\n",
    "    return svr_preds\n",
    "\n",
    "\n",
    "def gbc_svr_predict2(gbc, train_x, train_y, test_x):\n",
    "    pred_labels, pred_probs = gbc_svr_predict_part2(gbc, train_x, train_y, test_x, features.feature_pair_sub_list,\n",
    "                                                    features.feature_pair_plus_list,\n",
    "                                                    features.feature_pair_mul_list, features.feature_pair_divide_list,\n",
    "                                                    features.feature_pair_sub_mul_list,\n",
    "                                                    features.feature_pair_sub_list_sf,\n",
    "                                                    features.feature_pair_plus_list2)\n",
    "    return pred_labels, pred_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invoke the function gbc_gbr_predict_part\n",
    "def gbc_gbr_predict(gbc, train_x, train_y, test_x):\n",
    "    gbr_preds = gbc_gbr_predict_part(gbc, train_x, train_y, test_x, features.feature_pair_sub_list,\n",
    "                                     features.feature_pair_plus_list, features.feature_pair_mul_list,\n",
    "                                     features.feature_pair_divide_list, features.feature_pair_sub_mul_list,\n",
    "                                     features.feature_pair_sub_list2)\n",
    "    return gbr_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "class features:\n",
    "    import scipy.stats as stats\n",
    "    import random\n",
    "\n",
    "    feature_pair_sub_list = [[520, 521], [271, 521], [271, 520], [67, 466], [623, 664], [7, 536], [66, 529], [531, 532], [561, 562], [248, 602], [570, 571], [218, 766], [64, 765], [208, 590], [423, 660], [312, 463], [290, 592], [621, 755], [52, 311], [65, 422], [350, 656], [278, 420], [320, 633], [507, 761], [0, 341], [139, 665], [10, 724], [53, 319], [367, 698], [279, 421], [9, 358], [48, 287], [375, 653], [397, 728], [197, 666], [38, 295], [402, 758], [403, 757], [549, 584], [238, 258], [296, 526], [586, 607], [291, 591], [62, 289], [16, 288], [581, 589], [8, 380], [655, 683], [58, 582]]\n",
    "    feature_pair_plus_list = [[466, 529], [664, 759], [602, 766], [64, 665], [279, 590], [397, 592], [311, 621], [248, 755], [660, 768], [218, 666], [65, 278], [549, 607], [16, 402], [53, 757], [463, 526], [197, 312], [507, 762], [320, 619], [367, 380], [10, 350], [62, 401], [52, 756], [610, 633], [0, 656], [319, 758], [38, 50], [288, 296], [67, 584], [48, 611], [422, 724], [249, 591], [58, 686], [287, 295], [341, 589], [208, 728], [66, 508], [44, 605], [4, 358], [9, 695]]\n",
    "    feature_pair_mul_list = [[466, 529], [621, 664], [159, 626], [599, 602], [213, 607], [209, 218], [433, 463], [16, 665], [619, 766], [158, 625], [558, 605], [64, 248], [402, 660], [583, 606], [53, 279], [595, 596], [367, 590], [592, 633], [52, 278], [65, 350], [10, 38], [526, 644], [42, 397], [23, 666], [401, 758], [67, 73], [54, 589], [507, 549], [358, 591], [423, 610], [250, 312], [311, 755], [66, 353], [611, 732], [645, 765], [1, 320], [88, 341], [319, 757], [286, 296], [375, 403], [48, 509], [203, 581], [422, 655], [62, 87], [283, 622], [627, 724], [258, 686], [168, 268], [0, 197], [282, 646], [420, 656]]\n",
    "    feature_pair_divide_list = [[712, 664], [466, 726], [463, 6], [539, 529], [602, 507], [660, 653], [64, 208], [766, 630], [590, 197], [592, 0], [100, 145], [218, 3], [65, 9], [526, 525], [607, 643], [410, 647], [610, 144], [420, 278], [311, 10], [1, 279], [468, 462], [472, 444], [248, 652], [665, 510], [583, 143], [621, 59], [656, 52], [397, 509], [196, 700], [403, 401], [655, 54], [350, 518], [312, 672], [619, 615], [591, 187], [675, 671], [666, 157], [341, 8], [367, 217], [445, 402], [67, 637], [375, 654], [379, 716], [754, 549], [320, 517], [599, 596], [589, 267], [673, 718], [422, 400], [620, 386], [680, 719], [755, 28], [765, 383], [674, 721], [358, 717], [676, 720], [66, 167], [633, 347], [681, 71]]\n",
    "    feature_pair_sub_mul_list=[[271, 521, 1], [520, 521, 622], [7, 536, 218], [656, 664, 619], [664, 712, 621], [664, 719, 278], [664, 731, 401], [67, 466, 529], [466, 726, 65], [466, 754, 591], [664, 671, 402], [71, 664, 403], [342, 664, 280], [351, 664, 421], [623, 664, 666], [359, 664, 440], [368, 664, 510], [376, 664, 279], [414, 664, 420], [466, 539, 589], [424, 664, 665], [33, 664, 397], [466, 559, 10], [405, 664, 422], [664, 760, 439], [39, 664, 509], [55, 664, 620], [49, 664, 672], [66, 466, 549], [634, 664, 141], [267, 466, 433], [664, 703, 328], [60, 466, 64], [386, 664, 399], [38, 466, 633], [128, 466, 590], [615, 664, 511], [44, 466, 58], [107, 466, 157], [62, 466, 48], [7, 546, 248], [314, 507, 466], [531, 532, 209], [59, 664, 70], [126, 466, 367], [45, 664, 197], [246, 466, 118], [7, 556, 607], [117, 466, 592], [265, 466, 350], [97, 466, 156], [664, 702, 88], [7, 566, 258], [53, 466, 52], [215, 766, 466], [466, 584, 108], [236, 466, 72], [116, 466, 358], [61, 466, 56], [664, 704, 87], [311, 466, 28], [176, 466, 206], [29, 466, 631], [255, 466, 341], [247, 466, 0], [260, 466, 245], [664, 708, 98], [664, 696, 447], [664, 701, 601], [256, 466, 337], [664, 751, 431], [43, 466, 42], [46, 466, 36], [466, 602, 438], [40, 466, 717], [106, 466, 226], [319, 466, 758], [664, 697, 432], [410, 466, 686], [466, 741, 8], [250, 466, 235], [664, 693, 74], [86, 466, 630], [664, 752, 166], [18, 466, 632], [561, 562, 64], [313, 466, 54], [466, 746, 210], [466, 635, 9], [19, 466, 57], [63, 466, 517], [11, 466, 37], [237, 466, 216], [14, 466, 365], [664, 738, 127], [12, 466, 506], [96, 466, 338], [664, 709, 167], [326, 466, 764], [121, 466, 382], [47, 466, 15], [320, 466, 16], [664, 747, 227], [257, 466, 659], [186, 466, 139], [77, 529, 463], [321, 466, 375], [177, 466, 147], [187, 466, 217], [185, 466, 195], [294, 466, 660], [312, 466, 23], [664, 698, 225], [7, 575, 605], [266, 466, 650], [318, 466, 651], [664, 710, 599], [664, 739, 240], [17, 466, 348], [466, 582, 276], [664, 700, 441], [664, 694, 230], [404, 664, 393], [664, 748, 515], [570, 571, 64], [466, 767, 380], [287, 466, 356], [466, 706, 196], [295, 466, 339], [664, 740, 670], [664, 711, 627], [111, 466, 78], [50, 466, 654], [445, 466, 146], [448, 602, 768], [664, 699, 753], [664, 749, 277], [296, 466, 723], [406, 664, 394], [664, 695, 518], [188, 466, 190], [466, 587, 735], [466, 750, 285], [286, 466, 759], [664, 742, 220], [289, 466, 643], [165, 466, 366], [407, 664, 392], [155, 466, 21], [288, 466, 349], [109, 466, 175], [664, 743, 661], [664, 737, 207], [30, 607, 213], [664, 761, 3], [259, 466, 249], [178, 466, 373], [81, 466, 652], [322, 664, 101], [323, 664, 137], [238, 466, 27], [324, 664, 428], [325, 664, 91], [180, 466, 25], [68, 466, 653], [430, 466, 646], [79, 466, 357], [140, 664, 396], [327, 602, 745], [302, 466, 340], [31, 607, 558], [315, 664, 644], [32, 607, 214], [34, 607, 241], [35, 607, 242], [607, 668, 243], [607, 690, 244], [607, 691, 551], [607, 692, 552], [664, 730, 669], [310, 602, 655], [408, 664, 395], [297, 466, 26], [2, 602, 744], [526, 602, 757], [100, 602, 755], [160, 466, 24], [610, 766, 208], [189, 466, 170], [179, 466, 150], [316, 664, 138], [586, 607, 624], [317, 664, 614], [119, 602, 763], [526, 765, 645], [149, 466, 73], [99, 602, 756], [229, 602, 762], [110, 466, 400], [370, 664, 5], [602, 736, 732], [455, 463, 612], [664, 667, 270], [120, 602, 682], [466, 502, 595], [148, 466, 374], [228, 602, 681], [305, 602, 718], [611, 766, 516], [423, 660, 4], [89, 602, 369], [219, 602, 678], [239, 466, 715], [472, 602, 657], [168, 766, 353], [80, 466, 385], [158, 466, 22], [303, 602, 679], [90, 602, 720], [169, 602, 647], [664, 724, 362], [281, 660, 347], [159, 602, 680], [290, 664, 398], [492, 602, 683], [291, 664, 613], [292, 664, 20], [583, 602, 721], [293, 664, 384], [298, 664, 629], [508, 602, 675], [602, 606, 299], [602, 603, 300], [585, 602, 301], [463, 482, 381], [602, 714, 282], [304, 602, 360], [602, 713, 676], [588, 602, 673], [664, 728, 383], [568, 660, 538], [425, 660, 372], [221, 660, 222], [223, 660, 224], [572, 660, 577], [541, 660, 533], [542, 660, 251], [231, 660, 252], [232, 660, 253], [233, 660, 254], [234, 660, 261], [594, 602, 677], [593, 602, 722], [64, 364, 268], [306, 664, 355], [663, 664, 618], [563, 660, 553], [409, 466, 307], [450, 660, 13], [308, 664, 389], [345, 664, 729], [548, 660, 262], [474, 660, 346], [468, 660, 442], [82, 660, 628], [92, 660, 444], [309, 664, 637], [449, 660, 51], [93, 660, 462], [94, 660, 471], [487, 660, 596], [83, 660, 443], [486, 660, 598], [95, 660, 436], [84, 660, 597], [85, 660, 6], [608, 664, 275], [477, 660, 361], [484, 660, 454], [478, 660, 481], [496, 660, 491], [497, 660, 501], [102, 660, 263], [103, 660, 264], [104, 660, 75], [105, 660, 434], [724, 727, 642], [476, 660, 636], [112, 660, 354], [457, 660, 498], [113, 660, 363]]\n",
    "    feature_pair_sub_list2=[[520, 521], [271, 521], [271, 520], [67, 466], [623, 664], [7, 536], [66, 529], [531, 532], [561, 562], [248, 602], [570, 571], [218, 766], [64, 765], [208, 590], [423, 660], [312, 463], [290, 592], [621, 755], [52, 311], [65, 422], [350, 656], [278, 420], [320, 633], [507, 761], [0, 341], [139, 665], [10, 724], [53, 319], [367, 698], [279, 421], [9, 358], [48, 287], [375, 653], [397, 728], [197, 666], [38, 295], [402, 758], [403, 757], [549, 584], [238, 258], [296, 526], [586, 607], [291, 591], [62, 289], [16, 288], [581, 589], [8, 380], [655, 683], [58, 582], [54, 587], [377, 657], [44, 81], [292, 735], [610, 706], [177, 401], [109, 249], [187, 756], [425, 768], [293, 723], [61, 631], [365, 730], [378, 658], [100, 239], [37, 47], [12, 632], [313, 318], [541, 542], [268, 718], [36, 46], [28, 700], [306, 763], [79, 611], [361, 722], [727, 762], [11, 506], [588, 686], [99, 267], [360, 721], [717, 759], [431, 432], [679, 720], [508, 605], [282, 391], [285, 745], [23, 519], [445, 599], [379, 716], [250, 255], [176, 326], [4, 314], [348, 671], [110, 229], [166, 167], [15, 29], [230, 235], [585, 764], [198, 283], [157, 650], [219, 767], [307, 744], [308, 315], [309, 316], [400, 743], [496, 497], [455, 472], [317, 669], [413, 689], [87, 128], [196, 327], [281, 356], [488, 498], [92, 102], [93, 103], [94, 104], [95, 105], [410, 736], [257, 266], [373, 652], [642, 732], [228, 441], [17, 137], [42, 179], [704, 712], [719, 751], [107, 305], [408, 622], [88, 118], [731, 752], [220, 265], [3, 200], [71, 339], [56, 178], [82, 484], [494, 504], [212, 516], [342, 697], [351, 404], [353, 370], [13, 132], [337, 619], [628, 742], [359, 702], [368, 709], [651, 737], [376, 708], [414, 638], [424, 710], [240, 245], [33, 406], [405, 693], [637, 760], [385, 614], [39, 73], [55, 711], [49, 407], [286, 634], [346, 371], [389, 618], [160, 195], [259, 749], [45, 59], [645, 646], [502, 699], [40, 635], [492, 695], [347, 740], [274, 464], [111, 175], [215, 386], [149, 190], [615, 629], [272, 461], [146, 748], [449, 450], [338, 703], [430, 739], [310, 747], [459, 499], [18, 19], [277, 694], [476, 477], [148, 150], [302, 322], [323, 382], [14, 324], [57, 325], [517, 738], [43, 701], [336, 418], [465, 489], [138, 276], [83, 468], [448, 696], [205, 426], [294, 298], [299, 753], [300, 511], [127, 301], [144, 214], [204, 467], [608, 609], [170, 741], [143, 734], [133, 398], [191, 675], [192, 680], [269, 284], [237, 256], [80, 155], [193, 678], [194, 673], [130, 676], [21, 543], [134, 202], [142, 681], [662, 663], [363, 667], [135, 273], [331, 332], [674, 677], [112, 474], [199, 682], [275, 412], [168, 217], [345, 362], [460, 470], [171, 563], [490, 500], [247, 559], [172, 548], [366, 558], [173, 231], [174, 232], [270, 411], [181, 514], [161, 233], [162, 234], [169, 189], [343, 352], [211, 213], [84, 486], [344, 687], [85, 487], [182, 369], [163, 568], [164, 572], [512, 513], [183, 577], [349, 533], [612, 684], [184, 525], [151, 546], [152, 556], [153, 566], [154, 575], [27, 553], [446, 705], [384, 613], [515, 729], [5, 644], [25, 451], [86, 225], [241, 357], [221, 251], [222, 252], [223, 253], [224, 254], [493, 503], [552, 672], [26, 242], [180, 216], [523, 535], [243, 340], [24, 244], [261, 538], [89, 90], [262, 374], [263, 630], [264, 715], [428, 551], [117, 156], [201, 524], [51, 433], [60, 419], [522, 528], [647, 670], [627, 733], [131, 544], [534, 564], [434, 596], [545, 555], [78, 573], [77, 554], [185, 364], [136, 520], [495, 505], [435, 597], [2, 390], [113, 457], [120, 478], [114, 594], [115, 321], [603, 604], [260, 565], [303, 574], [547, 595], [209, 392], [396, 557], [537, 567], [576, 639], [41, 560], [165, 518], [297, 550], [96, 121], [122, 482], [119, 123], [91, 124], [116, 125], [437, 600], [569, 578], [30, 415], [746, 750], [76, 443], [393, 643], [453, 469], [456, 483], [442, 725], [74, 438], [203, 527], [479, 480], [98, 540], [458, 485], [141, 661], [31, 236], [394, 620], [32, 640], [473, 475], [34, 354], [452, 462], [713, 714], [72, 330], [659, 726], [580, 648], [75, 707], [35, 427], [329, 754], [97, 108], [186, 668], [395, 399], [6, 471], [246, 690], [510, 593], [101, 444], [439, 691], [20, 688], [454, 692], [106, 481], [147, 491], [126, 501], [436, 583], [416, 530], [226, 227], [598, 606], [383, 685], [50, 69], [509, 539], [328, 624], [68, 654], [158, 159], [63, 641], [304, 372], [355, 649], [280, 636], [188, 333], [625, 626], [1, 417], [206, 440], [22, 387], [129, 429], [334, 381], [70, 616], [145, 335], [140, 210]]\n",
    "    feature_pair_plus_list2=[[466, 529], [664, 759], [602, 766], [64, 665], [279, 590], [397, 592], [311, 621], [248, 755], [660, 768], [218, 666], [65, 278], [549, 607], [16, 402], [53, 757], [463, 526], [197, 312], [507, 762], [320, 619], [367, 380], [10, 350], [62, 401], [52, 756], [610, 633], [0, 656], [319, 758], [38, 50], [288, 296], [67, 584], [48, 611], [422, 724], [249, 591], [58, 686], [287, 295], [341, 589], [208, 728], [66, 508], [44, 605], [4, 358], [9, 695], [54, 99], [100, 258], [425, 577], [3, 655], [581, 765], [8, 694], [519, 658], [267, 582], [587, 706], [290, 420], [717, 730], [282, 561], [177, 238], [36, 326], [727, 763], [423, 543], [599, 631], [283, 657], [516, 689], [235, 764], [109, 187], [291, 608], [137, 767], [51, 292], [214, 721], [293, 346], [200, 268], [157, 289], [219, 229], [623, 653], [281, 533], [570, 732], [138, 745], [314, 667], [663, 761], [315, 421], [375, 400], [565, 722], [228, 305], [514, 718], [79, 176], [316, 510], [280, 317], [212, 720], [652, 671], [110, 441], [313, 403], [41, 744], [284, 465], [251, 646], [175, 632], [209, 743], [179, 196], [390, 712], [364, 622], [28, 723], [506, 650], [37, 195], [571, 683], [211, 426], [42, 170], [132, 677], [640, 742], [71, 643], [662, 737], [472, 736], [682, 705], [265, 559], [5, 332], [525, 680], [536, 678], [546, 675], [69, 342], [15, 318], [56, 255], [556, 673], [641, 719], [545, 716], [700, 729], [298, 609], [669, 735], [299, 410], [80, 239], [459, 734], [300, 354], [141, 749], [440, 731], [377, 513], [107, 121], [351, 419], [140, 359], [368, 439], [376, 509], [372, 414], [381, 424], [33, 217], [405, 518], [636, 760], [46, 150], [461, 578], [216, 257], [39, 445], [55, 139], [49, 399], [620, 634], [149, 156], [74, 699], [133, 547], [604, 740], [460, 687], [189, 386], [207, 615], [90, 748], [353, 703], [17, 259], [418, 512], [178, 739], [432, 698], [301, 747], [322, 404], [352, 535], [59, 323], [324, 407], [325, 408], [417, 464], [523, 684], [455, 738], [435, 676], [87, 190], [601, 701], [76, 681], [345, 702], [45, 186], [406, 752], [447, 704], [237, 697], [167, 751], [159, 696], [369, 453], [148, 693], [411, 733], [285, 365], [81, 588], [467, 616], [603, 709], [206, 708], [710, 741], [146, 327], [348, 638], [247, 711], [134, 524], [647, 679], [443, 674], [451, 558], [271, 378], [412, 434], [310, 337], [450, 541], [449, 542], [82, 231], [83, 232], [84, 233], [85, 234], [135, 335], [213, 437], [477, 566], [476, 575], [7, 112], [478, 552], [113, 261], [114, 262], [115, 263], [241, 457], [361, 574], [427, 528], [111, 128], [488, 548], [360, 520], [242, 494], [484, 562], [343, 521], [252, 487], [253, 486], [92, 254], [356, 642], [553, 624], [93, 264], [94, 243], [95, 244], [415, 557], [88, 302], [468, 551], [215, 502], [198, 391], [446, 544], [362, 563], [585, 726], [495, 560], [573, 725], [30, 245], [452, 564], [370, 572], [474, 568], [339, 645], [379, 555], [483, 567], [23, 205], [522, 554], [538, 654], [473, 576], [75, 413], [155, 168], [504, 531], [498, 532], [166, 180], [344, 480], [493, 537], [122, 221], [123, 222], [124, 223], [125, 224], [534, 600], [492, 651], [236, 260], [19, 127], [57, 392], [485, 569], [31, 586], [86, 108], [14, 511], [505, 550], [225, 595], [330, 442], [202, 456], [61, 63], [126, 246], [6, 329], [458, 540], [47, 396], [306, 659], [334, 475], [21, 644], [490, 648], [89, 750], [527, 685], [43, 627], [479, 707], [12, 433], [158, 227], [60, 393], [444, 625], [294, 753], [366, 672], [18, 185], [371, 628], [596, 626], [389, 470], [11, 169], [210, 307], [188, 308], [309, 328], [40, 539], [203, 688], [22, 454], [374, 481], [1, 491], [438, 501], [436, 639], [387, 530], [431, 598], [129, 489], [349, 373], [469, 597], [70, 462], [72, 471], [333, 649], [276, 338], [68, 226], [32, 713], [409, 496], [429, 497], [102, 304], [103, 256], [104, 118], [105, 355], [266, 515], [204, 416], [106, 606], [363, 428], [77, 382], [27, 171], [29, 635], [25, 172], [277, 448], [173, 357], [117, 250], [26, 174], [331, 499], [73, 637], [230, 240], [272, 503], [116, 160], [142, 269], [2, 78], [145, 500], [714, 754], [161, 340]]\n",
    "    feature_pair_sub_list_sf=[[271, 521], [520, 521], [271, 520], [592, 608], [238, 248], [186, 766], [60, 311], [176, 218], [9, 397], [248, 750], [90, 529], [396, 766], [660, 724], [108, 507], [107, 660], [65, 320], [577, 660], [271, 521], [10, 455], [64, 126], [56, 312], [664, 757], [518, 590], [511, 602], [65, 298], [259, 665], [12, 590], [529, 582], [291, 589], [187, 660], [296, 526], [64, 159], [170, 320], [64, 724], [65, 187], [508, 665], [298, 660], [358, 752], [592, 706], [25, 664], [397, 619], [466, 691], [52, 664], [665, 736], [350, 609], [665, 666], [664, 669], [248, 602], [66, 666], [118, 463], [311, 590], [64, 346], [309, 590], [312, 506], [319, 611], [660, 750], [65, 185], [706, 757], [764, 766], [271, 520], [394, 466], [150, 319], [367, 636], [611, 621], [64, 265], [186, 602], [10, 509], [89, 350], [590, 605], [312, 508], [101, 311], [358, 710], [399, 591], [664, 756], [507, 699], [229, 312], [98, 466], [661, 757], [165, 463], [62, 350], [328, 592], [586, 660], [68, 665], [156, 350], [633, 664], [218, 713], [311, 607], [187, 757], [511, 766], [338, 466], [559, 664], [63, 664], [318, 507], [509, 664], [659, 660], [64, 753], [249, 664], [169, 320], [10, 209], [584, 660], [422, 665], [394, 664], [19, 590], [341, 703], [101, 466], [231, 660], [41, 64], [38, 590], [350, 445], [7, 546], [350, 711], [631, 664], [188, 664], [65, 177], [100, 296], [410, 660], [50, 766], [287, 766], [187, 665], [24, 660], [160, 350], [34, 507], [303, 529], [472, 660], [421, 422], [549, 621], [81, 466], [660, 736], [61, 590], [664, 668], [515, 602], [99, 350], [260, 350], [246, 466], [71, 664], [660, 663], [302, 602], [220, 664], [310, 350], [259, 602], [431, 660], [558, 660], [120, 766], [665, 761], [64, 91], [179, 248], [36, 664], [664, 735], [16, 466], [10, 237], [107, 766], [66, 592], [592, 691], [338, 602], [177, 466], [33, 367], [350, 433], [64, 403], [220, 602], [12, 529], [106, 664], [65, 100], [216, 602], [507, 627], [656, 696], [196, 529], [373, 660], [350, 761], [529, 713], [466, 593], [77, 660], [86, 397], [305, 367], [63, 367], [638, 664], [175, 665], [305, 466], [72, 367], [520, 521], [220, 660], [64, 510], [96, 367], [10, 724], [367, 706], [248, 287], [185, 397], [64, 621], [303, 466], [91, 592], [466, 664], [86, 590], [438, 507], [64, 754], [65, 539], [42, 590], [341, 342], [36, 590], [176, 529], [286, 592], [150, 279], [350, 714], [65, 656], [38, 766], [656, 752], [431, 664], [583, 665], [472, 664], [197, 466], [726, 766], [592, 692], [656, 660], [266, 507], [180, 592], [660, 758], [367, 608], [448, 466], [96, 590], [445, 592], [350, 754], [287, 466], [229, 766], [116, 397], [260, 590], [466, 585], [87, 279], [57, 665], [187, 367], [37, 529], [10, 37], [64, 208], [602, 754], [186, 279], [15, 664], [65, 661], [86, 463], [196, 507], [45, 664], [67, 690], [65, 206], [267, 591], [466, 515], [397, 746], [294, 766], [237, 367], [589, 592], [502, 766], [37, 320], [367, 763], [507, 729], [397, 627], [109, 755], [64, 90], [180, 466], [65, 604], [257, 766], [207, 592], [168, 602], [176, 248], [506, 660], [276, 397], [636, 660], [664, 764], [179, 463], [100, 466], [457, 660], [420, 660], [250, 590], [255, 602], [638, 660], [367, 509], [40, 279], [507, 609], [98, 766], [603, 665], [318, 529], [64, 401], [126, 590], [166, 590], [218, 508], [393, 507], [63, 766], [276, 507], [41, 664], [341, 761], [311, 714], [111, 664], [169, 766], [341, 376], [10, 80], [97, 466], [552, 660], [664, 703], [295, 529], [664, 692], [323, 664], [250, 602], [96, 766], [287, 602], [219, 466], [79, 660], [585, 766], [250, 529], [313, 665], [590, 659], [278, 420], [32, 466], [10, 706], [147, 350], [10, 311], [289, 766], [43, 466], [10, 585], [246, 463], [350, 712], [10, 309], [397, 441], [432, 664], [664, 693], [583, 664], [29, 602], [35, 311], [235, 660], [41, 507], [397, 668], [325, 350], [466, 746], [342, 664], [67, 607], [238, 757], [63, 590], [52, 296], [32, 660], [265, 463], [15, 312], [397, 447], [227, 602], [65, 394], [10, 109], [219, 660], [665, 767], [110, 466], [88, 660], [664, 742], [96, 397], [586, 664], [664, 694], [358, 615], [298, 590], [279, 296], [65, 750], [65, 101], [185, 311], [64, 539], [52, 288], [72, 664], [366, 664], [218, 311], [117, 664], [326, 766], [466, 590], [592, 754], [54, 664], [226, 529], [65, 447], [74, 665], [507, 753], [279, 744], [219, 592], [67, 529], [91, 602], [111, 367], [664, 738], [279, 316], [249, 665], [65, 757], [350, 763], [602, 714], [305, 660], [8, 367], [110, 367], [8, 375], [664, 736], [15, 602], [137, 665], [65, 313], [219, 507], [345, 664], [312, 764], [236, 248], [116, 664], [350, 651], [32, 218], [664, 746], [302, 592], [220, 279], [99, 766], [195, 664], [278, 664], [265, 466], [227, 529], [650, 664], [206, 466], [296, 589], [218, 583], [664, 700], [279, 286], [62, 367], [248, 367], [64, 78], [209, 507], [218, 660], [279, 314], [518, 664], [10, 633], [515, 590], [139, 397], [507, 668], [666, 741], [64, 294], [259, 660], [350, 518], [2, 507], [341, 737], [394, 660], [65, 763], [10, 439], [150, 312], [250, 311], [41, 65], [139, 350], [64, 629], [66, 549], [313, 529], [118, 766], [401, 660], [107, 350], [167, 590], [64, 176], [180, 350], [67, 279], [262, 660], [660, 704], [15, 64], [67, 590], [240, 660], [466, 539], [77, 397], [466, 765], [350, 407], [297, 766], [127, 367], [306, 660], [100, 320], [9, 350], [47, 507], [338, 592], [53, 592], [367, 712], [51, 665], [65, 226], [91, 664], [10, 371], [65, 765], [10, 299], [65, 139], [438, 664], [230, 466], [367, 760], [62, 319], [155, 320], [33, 664], [121, 466], [260, 320], [602, 755], [327, 664], [582, 755], [549, 691], [16, 296], [508, 660], [403, 666], [35, 64], [16, 602], [108, 466], [238, 350], [88, 602], [590, 666], [65, 744], [64, 611], [240, 529], [320, 606], [107, 529], [159, 466], [177, 665], [186, 529], [108, 592], [664, 712], [52, 592], [515, 660], [47, 311], [197, 633], [425, 660], [396, 529], [150, 660]]\n",
    "\n",
    "    def find_corr_pairs_sub(train_x, train_y, eps=0.01):\n",
    "        feature_size = len(train_x[0])\n",
    "        feature_corr_list = []\n",
    "        for i in range(feature_size):\n",
    "            if i % 50 == 0:\n",
    "                print(i)\n",
    "            for j in range(feature_size):\n",
    "                if i < j:\n",
    "                    corr = stats.pearsonr(train_x[:,i] - train_x[:,j], train_y)\n",
    "                    if abs(corr[0]) < eps:\n",
    "                        continue\n",
    "                    feature_corr = (i, j, abs(corr[0]))\n",
    "                    feature_corr_list.append(feature_corr)\n",
    "\n",
    "        return feature_corr_list\n",
    "\n",
    "    def find_corr_pairs_plus(train_x, train_y, eps=0.01):\n",
    "        feature_size = len(train_x[0])\n",
    "        feature_corr_list = []\n",
    "        for i in range(feature_size):\n",
    "            if i % 50 == 0:\n",
    "                print(i)\n",
    "            for j in range(feature_size):\n",
    "                if i < j:\n",
    "                    corr = stats.pearsonr(train_x[:,i] + train_x[:,j], train_y)\n",
    "                    if abs(corr[0]) < eps:\n",
    "                        continue\n",
    "                    feature_corr = (i, j, corr[0])\n",
    "                    feature_corr_list.append(feature_corr)\n",
    "        return feature_corr_list\n",
    "\n",
    "    def find_corr_pairs_mul(train_x, train_y,eps=0.01):\n",
    "        feature_size = len(train_x[0])\n",
    "        feature_corr_list = []\n",
    "        for i in range(feature_size):\n",
    "            if i % 50 == 0:\n",
    "                print(i)\n",
    "            for j in range(feature_size):\n",
    "                if i < j:\n",
    "                    corr = stats.pearsonr(train_x[:,i] * train_x[:,j], train_y)\n",
    "                    if abs(corr[0]) < eps:\n",
    "                        continue\n",
    "                    feature_corr = (i, j, abs(corr[0]))\n",
    "                    feature_corr_list.append(feature_corr)\n",
    "\n",
    "        return feature_corr_list\n",
    "\n",
    "    def find_corr_pairs_divide(train_x, train_y, eps=0.01):\n",
    "        feature_size = len(train_x[0])\n",
    "        feature_corr_list = []\n",
    "        for i in range(feature_size):\n",
    "            if i % 50 == 0:\n",
    "                print(i)\n",
    "            for j in range(feature_size):\n",
    "                if i != j:\n",
    "                    try:\n",
    "                        res = train_x[:,i] / train_x[:,j]\n",
    "                        corr = stats.pearsonr(res, train_y)\n",
    "                        if abs(corr[0]) < eps:\n",
    "                            continue\n",
    "                        feature_corr = (i, j, abs(corr[0]))\n",
    "                        feature_corr_list.append(feature_corr)\n",
    "                    except ValueError:\n",
    "                        print('divide 0')\n",
    "\n",
    "        return feature_corr_list\n",
    "\n",
    "    def find_corr_pairs_sub_mul(train_x, train_y, sorted_corr_sub, eps=0.01):\n",
    "        feature_size = len(train_x[0])\n",
    "        feature_corr_list = []\n",
    "        for i in range(len(sorted_corr_sub)):\n",
    "            ind_i = sorted_corr_sub[i][0]\n",
    "            ind_j = sorted_corr_sub[i][1]\n",
    "            if i % 100 == 0:\n",
    "                print(i)\n",
    "            for j in range(feature_size):\n",
    "                if j != ind_i and j != ind_j :\n",
    "                    res = (train_x[:,ind_i] - train_x[:, ind_j]) * train_x[:,j]\n",
    "                    corr = stats.pearsonr(res, train_y)\n",
    "                    if abs(corr[0]) < eps:\n",
    "                        continue\n",
    "                    feature_corr = (ind_i, ind_j, j, corr[0])\n",
    "                    feature_corr_list.append(feature_corr)\n",
    "        return feature_corr_list\n",
    "\n",
    "    def get_distinct_feature_pairs(sorted_corr_list):\n",
    "        distinct_list = []\n",
    "        dis_ind = {}\n",
    "        for i in range(len(sorted_corr_list)):\n",
    "            if sorted_corr_list[i][0] not in dis_ind and sorted_corr_list[i][1] not in dis_ind:\n",
    "                dis_ind[sorted_corr_list[i][0]] = 1\n",
    "                dis_ind[sorted_corr_list[i][1]] = 1\n",
    "                distinct_list.append(sorted_corr_list[i])\n",
    "        return distinct_list\n",
    "\n",
    "    def get_distinct_feature_pairs2(sorted_corr_list):\n",
    "        distinct_list = []\n",
    "        dis_ind = {}\n",
    "        for sorted_corr in sorted_corr_list:\n",
    "            cnt = 0\n",
    "            for i in range(3):\n",
    "                if sorted_corr[i] in dis_ind:\n",
    "                    cnt = cnt + 1\n",
    "            if cnt > 1:\n",
    "                continue\n",
    "            for i in range(3):\n",
    "                dis_ind[sorted_corr[i]] = 1\n",
    "            distinct_list.append(sorted_corr)\n",
    "        return distinct_list\n",
    "\n",
    "    def get_feature_pair_sub_list(train_x, train_y, eps=0.01):\n",
    "        sub_list = find_corr_pairs_sub(train_x, train_y, eps)\n",
    "        sub_list2 = [corr for corr in sub_list if abs(corr[2])>eps]\n",
    "        sorted_sub_list = sorted(sub_list2, key=lambda corr:abs(corr[2]), reverse=True)\n",
    "        dist_sub_list = get_distinct_feature_pairs(sorted_sub_list)\n",
    "        dist_sub_list2 = [[corr[0], corr[1]] for corr in dist_sub_list]\n",
    "        feature_pair_sub_list = [[520, 521], [271, 521], [271, 520]]\n",
    "        feature_pair_sub_list.extend(dist_sub_list2[1:])\n",
    "        return feature_pair_sub_list\n",
    "\n",
    "    def get_feature_pair_plus_list(train_x, train_y, eps=0.01):\n",
    "        plus_list = find_corr_pairs_plus(train_x, train_y, eps)\n",
    "        plus_list2 = [corr for corr in plus_list if abs(corr[2])>eps]\n",
    "        sorted_plus_list = sorted(plus_list2, key=lambda corr:abs(corr[2]), reverse=True)\n",
    "        feature_pair_plus_list = get_distinct_feature_pairs(sorted_plus_list)\n",
    "        feature_pair_plus_list = [[corr[0],corr[1]] for corr in feature_pair_plus_list]\n",
    "        return feature_pair_plus_list\n",
    "\n",
    "    def get_feature_pair_mul_list(train_x, train_y, eps=0.01):\n",
    "        mul_list = find_corr_pairs_mul(train_x, train_y, eps)\n",
    "        mul_list2 = [corr for corr in mul_list if abs(corr[2])>eps]\n",
    "        sorted_mul_list = sorted(mul_list2, key=lambda corr:abs(corr[2]), reverse=True)\n",
    "        feature_pair_mul_list = get_distinct_feature_pairs(sorted_mul_list)\n",
    "        feature_pair_mul_list = [[corr[0],corr[1]] for corr in feature_pair_mul_list]\n",
    "        return feature_pair_mul_list\n",
    "\n",
    "    def get_feature_pair_divide_list(train_x, train_y, eps=0.01):\n",
    "        divide_list = find_corr_pairs_divide(train_x, train_y, eps)\n",
    "        divide_list2 = [corr for corr in divide_list if abs(corr[2])>eps]\n",
    "        sorted_divide_list = sorted(divide_list2, key=lambda corr:abs(corr[2]), reverse=True)\n",
    "        feature_pair_divide_list = get_distinct_feature_pairs(sorted_divide_list)\n",
    "        feature_pair_divide_list = [[corr[0],corr[1]] for corr in feature_pair_divide_list]\n",
    "        return feature_pair_divide_list\n",
    "\n",
    "    def get_feature_pair_sub_mul_list(train_x, train_y, eps=0.01):\n",
    "        feature_pair_sub_list = get_feature_pair_sub_list(train_x, train_y, eps=0.01)\n",
    "        sub_mul_list = find_corr_pairs_sub_mul(train_x, train_y, feature_pair_sub_list, eps=0.01)\n",
    "        sub_mul_list2 = [corr for corr in sub_mul_list if abs(corr[3]) > eps]\n",
    "        sorted_sub_mul_list = sorted(sub_mul_list2, key=lambda corr:abs(corr[2]), reverse=True)\n",
    "        feature_pair_sub_mul_list = get_distinct_feature_pairs2(sorted_sub_mul_list)\n",
    "        feature_pair_sub_mul_list = [[corr[0], corr[1], corr[2]] for corr in feature_pair_sub_mul_list]\n",
    "        return feature_pair_sub_mul_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main Function\n",
    "**Load Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.00000e+00, 1.26000e+02, 1.00000e+01, ..., 0.00000e+00,\n",
       "        5.00000e+00, 0.00000e+00],\n",
       "       [2.00000e+00, 1.21000e+02, 1.00000e+01, ..., 0.00000e+00,\n",
       "        5.00000e+00, 0.00000e+00],\n",
       "       [3.00000e+00, 1.26000e+02, 1.00000e+01, ..., 0.00000e+00,\n",
       "        5.00000e+00, 0.00000e+00],\n",
       "       ...,\n",
       "       [1.05469e+05, 1.29000e+02, 9.00000e+00, ..., 0.00000e+00,\n",
       "        9.30000e+01, 0.00000e+00],\n",
       "       [1.05470e+05, 1.29000e+02, 9.00000e+00, ..., 0.00000e+00,\n",
       "        9.30000e+01, 0.00000e+00],\n",
       "       [1.05471e+05, 1.29000e+02, 9.00000e+00, ..., 0.00000e+00,\n",
       "        9.30000e+01, 0.00000e+00]])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arrTrainData = np.genfromtxt(open('train_v2.csv','rb'), delimiter=',', skip_header=1)\n",
    "arrTrainData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.27360000e+04,  1.29000000e+02,  9.00000000e+00,  4.98267063e-01,\n",
       "        2.20000000e+03,  4.00000000e+00,  7.65300000e+04,  2.29200000e+03,\n",
       "        1.78600000e+03,  1.28460000e+02,  1.29080000e+02,  1.10000000e+01,\n",
       "        7.70500000e-01,  7.54200000e-01,  1.67276400e+06,  7.45800000e-01,\n",
       "        7.36000000e-01,  5.39200000e-01,  7.61900000e-01,  8.18200000e-01,\n",
       "        7.98200000e-01,  2.23394688e+09,  9.10000000e+01,  6.50000000e+01,\n",
       "        1.11513600e+06,  9.47800000e+01,  9.40000000e+01,  1.08000000e+02,\n",
       "        9.90000000e+01,  9.20000000e+01,  9.25500000e-01,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  1.10000000e+01,  0.00000000e+00,\n",
       "        0.00000000e+00,  7.16000000e-01,  7.30510000e-01,  7.03467000e-01,\n",
       "        1.10000000e+01,  8.68830000e-02, -7.90845500e-01,  8.00850000e-01,\n",
       "        8.13320000e-01,  7.89750000e-01,  1.00000000e+01,  7.35600000e-01,\n",
       "        7.49780000e-01,  7.20633000e-01,  1.10000000e+01,  8.74050000e-02,\n",
       "       -8.47308000e-01,  5.27650000e-01,  5.37000000e-01,  5.15839000e-01,\n",
       "        1.10000000e+01,  7.81600000e-01,  7.93790000e-01,  7.70300000e-01,\n",
       "        1.00000000e+01,  9.10000000e-01,  9.20790000e-01,  9.03691000e-01,\n",
       "        7.53715000e-02,  7.82780000e+00,  1.10000000e+01,  0.00000000e+00,\n",
       "        0.00000000e+00,  3.28000000e-02,  0.00000000e+00,  9.00000000e+00,\n",
       "        1.10000000e+01,  6.00000000e+00,  5.71000000e+00,  6.51000000e+00,\n",
       "        1.54050000e+04,  1.46081000e+05,  9.40000000e-01,  1.40000000e+00,\n",
       "        1.00000000e-02,  0.00000000e+00,  9.00000000e-02,  1.94000000e+02,\n",
       "        1.94000000e+02,  1.94000000e+02,  1.94000000e+02,  2.90000000e-01,\n",
       "        1.42000000e+00,  1.74000000e+00,  1.00000000e-02,  0.00000000e+00,\n",
       "        9.00000000e-02,  2.00000000e+02,  2.00000000e+02,  2.00000000e+02,\n",
       "        2.00000000e+02,  2.90000000e-01,  1.41000000e+00,  1.73000000e+00,\n",
       "        1.00000000e-02,  0.00000000e+00,  9.00000000e-02,  2.10000000e+02,\n",
       "        2.10000000e+02,  2.10000000e+02,  2.10000000e+02,  2.90000000e-01,\n",
       "        1.41000000e+00,  1.73000000e+00,  8.00000000e-02,  0.00000000e+00,\n",
       "        1.80000000e-01,  3.92000000e+02,  3.92000000e+02,  3.92000000e+02,\n",
       "        3.92000000e+02,  3.80000000e-01,  1.18000000e+00,  1.38000000e+00,\n",
       "        2.30000000e-01,  1.00000000e-02,  2.80000000e-01,  6.04000000e+02,\n",
       "        6.04000000e+02,  6.04000000e+02,  6.04000000e+02,  4.50000000e-01,\n",
       "        9.90000000e-01,  1.18000000e+00,  3.42270000e+02,  8.72900000e+01,\n",
       "        6.95510000e+02,  1.60865200e+06,  1.56248210e+10,  4.48000000e+14,\n",
       "        1.83488390e+19,  2.47742000e+03,  2.21000000e+00,  3.66000000e+00,\n",
       "        5.95000000e+00,  5.65000000e+00,  6.37000000e+00,  3.48000000e+02,\n",
       "        3.21000000e+03,  3.63270000e+04,  4.66888500e+05,  8.90000000e-01,\n",
       "        1.30000000e+00,  1.00000000e-02,  0.00000000e+00,  9.00000000e-02,\n",
       "        5.00000000e+00,  5.00000000e+00,  5.00000000e+00,  5.00000000e+00,\n",
       "        2.90000000e-01,  1.37000000e+00,  1.67000000e+00,  1.00000000e-02,\n",
       "        0.00000000e+00,  9.00000000e-02,  5.00000000e+00,  5.00000000e+00,\n",
       "        5.00000000e+00,  5.00000000e+00,  2.90000000e-01,  1.37000000e+00,\n",
       "        1.67000000e+00,  1.00000000e-02,  0.00000000e+00,  9.00000000e-02,\n",
       "        5.00000000e+00,  5.00000000e+00,  5.00000000e+00,  5.00000000e+00,\n",
       "        2.90000000e-01,  1.39000000e+00,  1.69000000e+00,  7.00000000e-02,\n",
       "        0.00000000e+00,  1.90000000e-01,  1.00000000e+01,  1.00000000e+01,\n",
       "        1.00000000e+01,  1.00000000e+01,  3.90000000e-01,  1.14000000e+00,\n",
       "        1.34000000e+00,  2.30000000e-01,  1.00000000e-02,  2.90000000e-01,\n",
       "        1.50000000e+01,  1.50000000e+01,  1.50000000e+01,  1.50000000e+01,\n",
       "        4.50000000e-01,  9.50000000e-01,  1.15000000e+00,  3.08495000e+02,\n",
       "        8.46700000e+01,  5.83595000e+02,  3.37205000e+04,  1.09702376e+08,\n",
       "        4.66000000e+11,  2.23000000e+15,  1.18493500e+03,  1.46000000e+00,\n",
       "        1.90000000e+00,  6.02000000e+00,  5.74000000e+00,  6.55000000e+00,\n",
       "        1.17390000e+04,  1.10575000e+05,  1.33270950e+06,  1.87108905e+07,\n",
       "        4.28000000e+00,  9.60000000e-01,  1.40000000e+00,  1.00000000e-02,\n",
       "        0.00000000e+00,  9.00000000e-02,  1.55000000e+02,  1.55000000e+02,\n",
       "        1.55000000e+02,  1.55000000e+02,  2.80000000e-01,  1.43000000e+00,\n",
       "        1.76000000e+00,  1.00000000e-02,  0.00000000e+00,  1.00000000e-01,\n",
       "        1.66000000e+02,  1.66000000e+02,  1.66000000e+02,  1.66000000e+02,\n",
       "        2.90000000e-01,  1.40000000e+00,  1.72000000e+00,  1.00000000e-02,\n",
       "        0.00000000e+00,  1.00000000e-01,  1.65000000e+02,  1.65000000e+02,\n",
       "        1.65000000e+02,  1.65000000e+02,  2.90000000e-01,  1.40000000e+00,\n",
       "        1.70000000e+00,  8.00000000e-02,  0.00000000e+00,  1.80000000e-01,\n",
       "        3.19000000e+02,  3.19000000e+02,  3.19000000e+02,  3.19000000e+02,\n",
       "        3.90000000e-01,  1.17000000e+00,  1.38000000e+00,  2.20000000e-01,\n",
       "        1.00000000e-02,  2.80000000e-01,  4.90000000e+02,  4.90000000e+02,\n",
       "        4.90000000e+02,  4.90000000e+02,  4.50000000e-01,  9.90000000e-01,\n",
       "        1.18000000e+00,  3.27450000e+02,  8.09700000e+01,  7.05235000e+02,\n",
       "        1.25168200e+06,  1.00537344e+10,  2.36000000e+14,  8.67000000e+18,\n",
       "        2.23541500e+03,  2.06000000e+00,  3.28000000e+00,  5.75000000e+00,\n",
       "        5.50000000e+00,  5.76000000e+00,  6.30000000e+01,  5.24000000e+02,\n",
       "        5.21400000e+03,  5.64040000e+04,  3.31000000e+00,  8.10000000e-01,\n",
       "        1.00000000e-02,  0.00000000e+00,  9.00000000e-02,  1.00000000e+00,\n",
       "        1.00000000e+00,  1.00000000e+00,  1.00000000e+00,  2.90000000e-01,\n",
       "        1.00000000e-02,  0.00000000e+00,  9.00000000e-02,  1.00000000e+00,\n",
       "        1.00000000e+00,  1.00000000e+00,  1.00000000e+00,  2.90000000e-01,\n",
       "        1.00000000e-02,  0.00000000e+00,  9.00000000e-02,  1.00000000e+00,\n",
       "        1.00000000e+00,  1.00000000e+00,  1.00000000e+00,  2.80000000e-01,\n",
       "        7.00000000e-02,  0.00000000e+00,  2.10000000e-01,  2.00000000e+00,\n",
       "        2.00000000e+00,  2.00000000e+00,  2.00000000e+00,  4.00000000e-01,\n",
       "        2.50000000e-01,  1.00000000e-02,  3.30000000e-01,  4.00000000e+00,\n",
       "        4.00000000e+00,  4.00000000e+00,  4.00000000e+00,  4.60000000e-01,\n",
       "        8.40000000e-01,  1.12000000e+00,  3.02240000e+02,  9.17900000e+01,\n",
       "        5.30150000e+02,  6.52600000e+03,  1.47087520e+07,  3.75711340e+10,\n",
       "        1.10000000e+14,  8.76270000e+02,  1.20000000e+00,  1.47000000e+00,\n",
       "        9.22700000e+01,  9.40490000e+01,  9.11666700e+01,  1.10000000e+01,\n",
       "        1.00233800e+07,  1.02577429e+09,  1.02240150e+01, -8.61575000e-01,\n",
       "        1.24531000e+00,  9.10000000e+01,  9.30200000e+01,  8.93838500e+01,\n",
       "        1.10000000e+01,  8.36214951e+08,  9.94716000e+00, -8.70540000e-01,\n",
       "        1.24837500e+00,  1.04385000e+02,  1.07250000e+02,  1.02305000e+02,\n",
       "        1.10000000e+01,  1.48481659e+07,  1.78186665e+09,  1.77118200e+01,\n",
       "       -6.47040000e-01,  1.21894000e+00,  9.63650000e+01,  9.86200000e+01,\n",
       "        9.43425000e+01,  1.10000000e+01,  1.06395991e+09,  1.23876700e+01,\n",
       "       -7.86810000e-01,  1.23614000e+00,  6.25000000e+01,  6.44000000e+01,\n",
       "        6.33636000e+01,  1.10000000e+01,  4.94140000e+04,  3.64261500e+06,\n",
       "        2.73405160e+08,  1.58697000e+01, -8.99600000e-01,  1.25240000e+00,\n",
       "        2.18444043e+09,  2.22362499e+09,  2.02839973e+09,  1.10000000e+01,\n",
       "        1.51909713e+29,  3.54688871e+38,  4.41689354e+08, -3.40000000e-01,\n",
       "        1.22000000e+00,  4.93000000e-02,  4.77000000e-02,  6.22000000e-02,\n",
       "        8.00000000e-02,  5.87000000e-02,  5.00000000e+00,  2.43488102e+09,\n",
       "        3.79000000e-01,  4.00000000e+00,  5.25000000e-01,  5.20000000e-01,\n",
       "        4.96923000e-01,  5.38000000e+00,  1.10000000e+01,  3.52300000e+00,\n",
       "        2.65380000e+00,  2.14680000e+00,  5.18308000e-01,  1.17167550e+00,\n",
       "        5.77079000e+03,  6.44249000e+03,  3.40114000e+03,  1.10000000e+01,\n",
       "        1.76009291e+08,  9.89000000e+11,  5.83000000e+15,  2.17584000e+03,\n",
       "        0.00000000e+00,  5.76500000e+00,  5.76500000e+00,  5.78000000e+00,\n",
       "        6.26800000e+01,  1.10000000e+01,  4.07730000e+02,  2.76824000e+03,\n",
       "        1.87612900e+04,  9.96300000e-01,  5.26600000e-01,  1.24630000e+00,\n",
       "        6.51500000e+00,  6.50900000e+00,  3.52400000e-01,  1.54772400e+04,\n",
       "        1.07730570e+05,  2.29200000e+03,  7.21409920e+05,  6.75340000e+00,\n",
       "        5.91000000e+00,  5.80300000e+00,  2.31330000e+00,  1.37302800e+04,\n",
       "        1.01000970e+05,  2.09500000e+03,  1.03020000e+00,  8.73182650e+05,\n",
       "        6.41550000e+00,  1.56580000e+00,  1.94425000e+02,  1.94900000e+02,\n",
       "        5.60700000e+01,  2.20623000e+05,  2.83763820e+07,  2.29200000e+03,\n",
       "        3.10000000e-01,  4.14905851e+09,  9.83500000e+01,  6.46000000e+11,\n",
       "        4.71648220e+05,  1.81830277e+09,  1.93000000e+15,  2.28600000e+03,\n",
       "        3.60000000e-01,  2.38012043e+21,  7.93893880e+05,  1.16000000e+00,\n",
       "        3.03312423e+27,  1.74240000e+02,  6.95106000e+05,  2.79159010e+08,\n",
       "        2.28600000e+03,  2.70000000e-01,  1.26000000e+11,  3.01940000e+02,\n",
       "        6.13000000e+13,  3.93470000e+02,  3.93900000e+02,  1.13940000e+02,\n",
       "        4.51906000e+05,  1.19852034e+08,  2.29200000e+03,  3.40000000e-01,\n",
       "        3.54631051e+10,  1.98780000e+02,  1.13000000e+13,  2.01000000e+02,\n",
       "        2.01000000e+02,  5.82000000e+01,  2.31236000e+05,  3.14260420e+07,\n",
       "        2.29200000e+03,  3.40000000e-01,  4.73227028e+09,  1.00480000e+02,\n",
       "        7.63000000e+11,  2.10000000e+02,  2.10000000e+02,  6.08300000e+01,\n",
       "        2.42799000e+05,  3.41157470e+07,  2.29200000e+03,  3.50000000e-01,\n",
       "        5.41529180e+09,  1.03840000e+02,  9.12000000e+11,  7.88570000e-01,\n",
       "        9.02185000e+01,  1.50000000e-03,  5.89500000e+00,  5.79700000e+00,\n",
       "        2.31430000e+00,  1.02275300e+04,  7.31415400e+04,  1.63500000e+03,\n",
       "        1.04070000e+00,  6.04843900e+05,  6.31220000e+00,  1.56690000e+00,\n",
       "        5.79576555e+06,  1.25145921e+06,  1.25205794e+06,  3.58180810e+05,\n",
       "        1.13507827e+09,  9.59000000e+14,  1.78500000e+03,  3.60000000e-01,\n",
       "        9.03621023e+20,  6.21822660e+05,  1.16000000e+00,  9.03025368e+26,\n",
       "        1.55000000e+02,  1.55000000e+02,  4.51050000e+01,  1.35005000e+05,\n",
       "        1.40558150e+07,  1.78600000e+03,  1.64031110e+09,  7.81700000e+01,\n",
       "        1.16000000e+00,  2.06000000e+11,  1.66000000e+02,  1.66000000e+02,\n",
       "        4.82500000e+01,  1.44143500e+05,  1.58481050e+07,  1.78600000e+03,\n",
       "        1.99173842e+09,  8.14600000e+01,  1.16000000e+00,  2.67000000e+11,\n",
       "        1.65000000e+02,  1.65000000e+02,  4.75300000e+01,  1.42764000e+05,\n",
       "        1.54990535e+07,  1.78600000e+03,  1.90349506e+09,  8.09300000e+01,\n",
       "        1.16000000e+00,  2.48000000e+11,  3.19190000e+02,  3.19890000e+02,\n",
       "        9.21300000e+01,  2.79680500e+05,  6.05818795e+07,  1.78600000e+03,\n",
       "        1.47149697e+10,  1.58475000e+02,  3.80000000e+12,  4.90490000e+02,\n",
       "        4.90900000e+02,  1.41520000e+02,  4.21135000e+05,  1.36515028e+08,\n",
       "        1.78600000e+03,  5.01354077e+10,  2.42510000e+02,  1.97000000e+13,\n",
       "        1.00000000e-02,  0.00000000e+00,  4.00000000e-02,  2.57000000e-02,\n",
       "        8.00000000e-04,  7.50000000e-04,  1.35000000e-03,  1.15000000e-03,\n",
       "        1.00000000e-02,  0.00000000e+00,  5.52500000e+00,  1.10000000e+01,\n",
       "        5.50100000e+00,  1.10000000e+01, -3.00000000e-02, -1.00000000e-02,\n",
       "        0.00000000e+00,  1.48672400e+04,  1.01786891e+05,  2.28800000e+03,\n",
       "        9.18672000e-01,  7.46954322e+05,  6.41243800e+00,  1.47585300e+00,\n",
       "       -3.00000000e-02, -1.00000000e-02,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  1.10000000e-01,  2.00000000e-02,  0.00000000e+00,\n",
       "        0.00000000e+00,  2.38901868e+09,  2.42476804e+09,  2.21116432e+09,\n",
       "        1.10000000e+01,  1.94552184e+29,  5.03886005e+38,  5.00166558e+08,\n",
       "        4.40000000e-01,  3.90850000e-01,  4.60840000e-01,  5.56900000e+00,\n",
       "        1.10000000e+01,  5.30200000e+00,  5.81800000e+00,  7.02600000e+00,\n",
       "        3.87380000e-01,  9.34510000e-01,  1.27734000e+00,  9.06350000e+01,\n",
       "        7.70830000e-01,  7.82515000e-01,  7.61028600e-01,  1.10000000e+01,\n",
       "        1.08343800e-01, -6.92476800e-01,  1.23096940e+00, -1.70000000e-01,\n",
       "       -1.00000000e-01,  2.00000000e-02,  0.00000000e+00,  2.10000000e+01,\n",
       "        7.40000000e+01,  3.32900000e+01,  2.46000000e+01,  5.74000000e+02,\n",
       "        7.09250000e+04,  1.25582400e+07,  2.45329205e+09,  1.21000000e+00,\n",
       "        1.48000000e+00,  7.49900000e+01,  7.49500000e+01,  7.57846000e+01,\n",
       "        7.66000000e+02,  1.00000000e+01,  5.99240000e+04,  4.65245900e+06,\n",
       "        4.25960000e+00,  7.08000000e+02,  6.35000000e-02, -3.90000000e-01,\n",
       "       -1.50000000e-01,  2.40000000e+01,  5.00000000e+00,  5.00000000e-01,\n",
       "        6.00000000e+00,  0.00000000e+00,  2.00000000e+00,  5.57100000e+01,\n",
       "        1.10000000e+01,  4.40700000e+01,  9.62000000e+02,  9.82510000e+04,\n",
       "        9.03780000e+02,  8.75100000e+04,  8.54247244e+06,  1.07907000e+03,\n",
       "        1.25710391e+05,  9.53460000e+02,  9.77816390e+04,  1.01572987e+07,\n",
       "        6.89000000e+02,  2.83890930e+10,  7.48028921e+19,  2.52539000e-01,\n",
       "        2.62664920e+10,  6.37571083e+19,  3.43810000e+04,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  6.50200000e+00,  4.71060000e+00,\n",
       "        3.51420000e+00,  7.59495000e+00,  6.91310000e+00,  5.20280000e+00,\n",
       "        3.97830000e+00,  5.20350000e+00,  7.37970000e+00,  8.92770000e+00,\n",
       "        1.00000000e+01,  8.18200000e+00,  8.70483980e+06,  1.40044300e+00,\n",
       "        5.86848789e+06,  6.98960000e+00,  5.46394000e+00,  4.40263000e+00,\n",
       "        3.60559000e+00,  1.20000000e+01,  0.00000000e+00,  0.00000000e+00,\n",
       "        3.00000000e+00,  3.62031924e+08,  1.27250000e+02,  1.60100000e+03,\n",
       "        1.20000000e+01,  2.16239000e+05,  2.92179590e+07,  3.94942913e+09,\n",
       "        4.87000000e+00,  1.80000000e-01,  4.85085671e+06,  1.00000000e+00,\n",
       "        1.25000000e-01,  1.75000000e-01,  1.20000000e-01,  1.08000000e+00,\n",
       "        1.20000000e+01,  2.43630000e+02,  3.38000000e+00,  1.49863700e+04,\n",
       "        4.17280000e+00, -3.75100000e-01,  2.65200000e+00,  6.19280000e+00,\n",
       "        5.11345000e+00,  4.26195000e+00,  8.36370000e-02,  3.08160000e+00,\n",
       "        2.84070000e+00,  1.62700000e+00,  9.57600000e-01,  1.03506500e-01,\n",
       "        5.85580000e+00,  4.71150000e+00,  3.83000000e+00,  8.20080000e-02,\n",
       "        7.56490000e+00,  7.04550000e+00, -1.81800000e-01,  1.00000000e+00,\n",
       "       -5.00000000e-01, -4.80000000e-01, -4.80000000e-01, -5.03100000e-01,\n",
       "       -5.44000000e+00,  1.10000000e+01,  3.57000000e+00, -2.60000000e+00,\n",
       "        1.99000000e+00,  2.51800000e-01,  3.75400000e-01,  0.00000000e+00,\n",
       "        0.00000000e+00,  4.00000000e+01,  0.00000000e+00])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arrColMedian = np.nanmedian(arrTrainData, axis=0)\n",
    "arrColMedian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "inds = np.where(np.isnan(arrTrainData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrTrainData[inds] = np.take(arrColMedian, inds[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrTrainData[np.isinf(arrTrainData)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[126.        ,  10.        ,   0.6868419 , ...,   1.        ,\n",
       "          0.        ,   5.        ],\n",
       "       [121.        ,  10.        ,   0.78277593, ...,   1.        ,\n",
       "          0.        ,   5.        ],\n",
       "       [126.        ,  10.        ,   0.50007998, ...,   1.        ,\n",
       "          0.        ,   5.        ],\n",
       "       ...,\n",
       "       [129.        ,   9.        ,   0.24185807, ...,   1.        ,\n",
       "          0.        ,  93.        ],\n",
       "       [129.        ,   9.        ,   0.56971859, ...,   1.        ,\n",
       "          0.        ,  93.        ],\n",
       "       [129.        ,   9.        ,   0.40770742, ...,   1.        ,\n",
       "          0.        ,  93.        ]])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arrTrainX = arrTrainData[:,range(1, len(arrTrainData[0])-1)]\n",
    "arrTrainX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arrTrainY = arrTrainData[:,-1]\n",
    "arrTrainY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arrLabels = np.zeros(len(arrTrainY))\n",
    "arrLabels[arrTrainY > 0] = 1\n",
    "arrLabels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train Test Split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_fs = load_train_fs()\n",
    "#test_fs = load_test_fs()\n",
    "#train_x, train_y = train_type(train_fs)\n",
    "#test_x = test_type(test_fs)\n",
    "\n",
    "#train_data_x = np.array(df_data_xy.drop(['loss'], axis=1))\n",
    "#train_data_y = np.array(df_data_xy['loss'])\n",
    "\n",
    "#train_x = np.array(df_x_train)\n",
    "#train_y = np.array(df_y_train)\n",
    "#test_x  = np.array(df_x_test)\n",
    "#test_y  = np.array(df_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x length :  73829\n",
      "train_y length :  73829\n",
      "test_x length :  31642\n",
      "test_y length :  31642\n"
     ]
    }
   ],
   "source": [
    "train_x, test_x, train_y, test_y = train_test_split(arrTrainX, arrTrainY, test_size=.3, random_state=123)\n",
    "print('train_x length : ', len(train_x))\n",
    "print('train_y length : ', len(train_y))\n",
    "print('test_x length : ', len(test_x))\n",
    "print('test_y length : ', len(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_test_y = np.zeros(len(test_y))\n",
    "labels_test_y[test_y>0] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Engineering and Modeling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_minus_pair_list\n",
      "feature_plus_pair_list\n",
      "feature_mul_pair_list\n",
      "feature_divide_pair_list\n",
      "feature_pair_sub_mul_list\n"
     ]
    }
   ],
   "source": [
    "gbc = gbc_classify(train_x, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_minus_pair_list\n",
      "feature_plus_pair_list\n",
      "feature_mul_pair_list\n",
      "feature_divide_pair_list\n",
      "feature_pair_sub_mul_list\n",
      "feature_minus_pair_list\n",
      "feature_plus_pair_list\n",
      "feature_mul_pair_list\n",
      "feature_divide_pair_list\n",
      "feature_pair_sub_mul_list\n"
     ]
    }
   ],
   "source": [
    "#pred_labels,pred_probs = gbc_predict_part(gbc, train_x, train_y, test_x)\n",
    "pred_labels, pred_probs = gbc_predict_part(gbc, train_x, train_y, test_x, \n",
    "                                           features.feature_pair_sub_list,\n",
    "                                           features.feature_pair_plus_list,\n",
    "                                           features.feature_pair_mul_list, \n",
    "                                           features.feature_pair_divide_list,\n",
    "                                           features.feature_pair_sub_mul_list,\n",
    "                                           features.feature_pair_sub_list_sf,\n",
    "                                           features.feature_pair_plus_list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.99961141, 0.97675666, 0.99965342, ..., 0.96328357, 0.48889013,\n",
       "       0.99680774])"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_probs[pred_probs>0.2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- Accuracy Score ----------------------------------\n",
      "Accuracy Score :  0.9895\n",
      "\n",
      "----------------- Confusion Matrix --------------------------------\n",
      "[[28430   230]\n",
      " [  102  2880]]\n",
      "\n",
      "----------------- TP,FP,TN,FN -------------------------------------\n",
      "True  Positive :  2880\n",
      "False Positive :  230\n",
      "True  Negative :  28430\n",
      "False Negative :  102\n",
      "Number of Correct Predictions   (TP + TN) :  31310\n",
      "Number of Incorrect Predictions (FP + FN) :  332\n",
      "\n",
      "----------------- Precision/Recall/F1-Score -----------------------\n",
      "Precision          :  0.926\n",
      "Recall/Sensitivity :  0.9658\n",
      "F1 Score           :  0.9455\n",
      "\n",
      "----------------- Classification Report ---------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.99      0.99     28660\n",
      "         1.0       0.93      0.97      0.95      2982\n",
      "\n",
      "    accuracy                           0.99     31642\n",
      "   macro avg       0.96      0.98      0.97     31642\n",
      "weighted avg       0.99      0.99      0.99     31642\n",
      "\n",
      "\n",
      "----------------- ROC Curve ---------------------------------------\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------- ROC AUC Score -----------------------------------\n",
      "ROC AUC Score :  0.9981\n"
     ]
    }
   ],
   "source": [
    "# Evaluation Matrix\n",
    "#get_evaluation_matrices(test_y, arr_y_pred_gb, arr_y_pred_proba_def_gb)\n",
    "get_evaluation_matrices(labels_test_y, pred_labels, pred_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
